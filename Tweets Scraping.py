# -*- coding: utf-8 -*-
"""gettweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R5TiMeuB3OL0nLpFhzWN0zrx1B2DBMEx

The following code looks for tweets for covid 19 resources.
It takes the **area** and the **requirement** as input and produces a csv file with headers as following.

**['Info', 'Contact', 'Requirement', 'area', 'tweeted at']**
"""

import os
import tweepy as tw
import pandas as pd
import datetime
import re

consumer_key = 'xBtn4u0pUHd9KahAb6Oomunyv'
consumer_secret = 'Wo6PLH6AxvBFRrKx3xPpYm0Js6HLi48Dun6EGbWvXjXzbZn6FS'
access_token = '1394308063595614217-awM8gYyoizvXxbL2poa7bg0NK9kRaq'
access_token_secret = '8yAYb2HJY50oKkJ5ZS3Ngt4A9CFiePhKXCx2I1aYBjZpF'

keywords = ['verified Delhi (oxygen) -"not verified" -"unverified" -"needed" -"need" -"needs" -"required" -"require" -"requires" -"requirement" -"requirements"']

keys = 'verified Delhi (oxygen) -"not verified" -"unverified" -"needed" -"need" -"needs" -"required" -"require" -"requires" -"requirement" -"requirements"'

# getting input
requirement = input('what are you looking for?')
area = input('please enter the city to look for')

keys = keys.replace('Delhi', area)
keys = keys.replace('oxygen', requirement)

words = [keys]

auth = tw.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tw.API(auth)

tweets = []
tweeted_at = []


for i in tw.Cursor(api.search, q=words, tweet_mode='extended').items(500):
  tweets.append(i.full_text)
  tweeted_at.append(i.created_at)

tweets[0:5]

covid_resoures = pd.DataFrame({'tweet': tweets, 'tweeted at':tweeted_at})

covid_resoures.head()

covid_resoures = covid_resoures[~covid_resoures.tweet.str.contains('RT') ]

covid_resoures.reset_index(inplace=True, drop=True)

#getting contact number
contact_no = []
for tweet in covid_resoures['tweet']:
  match = re.search(r'\+?\d[\d -]{8,12}\d', tweet)
  if match:
    contact_no.append(match.group())
  else:
    contact_no.append('Not found')

#Cleaning the data
clean = []
for tweet in covid_resoures['tweet']:
  r = tweet.split('\n')
  use_r = []
  for word in r:
    if '#' not in word:
      if '@' not in word:
        use_r.append(word)
  keywords = ['verified Delhi (oxygen) -"not verified" -"unverified" -"needed" -"need" -"needs" -"required" -"require" -"requires" -"requirement" -"requirements"']
  needfinal = []
  for word in use_r:
    if word not in keywords:
      needfinal.append(word)
  clean.append(needfinal)

info = []
wrds = ['hospital','mr.','name','supplier']
for data in clean:
  append = False
  for d in data:
    if re.compile('|'.join(wrds),re.IGNORECASE).search(d.lower()):
      append = True
      info.append(d)
  if not append:
    info.append('Oxygen supplier')

details = pd.DataFrame({'Info': info, 'contact': contact_no, 'requirement': ['Oxygen']*len(info), 'Area': ['delhi']*len(info)})
details['tweeted at'] = covid_resoures['tweeted at']

new_details = details[(details['Info']!='Oxygen Supplier') & (details['contact']!='Not found')]

new_details.reset_index(inplace=True, drop=True)

new_details.to_csv('Probable resources.csv')